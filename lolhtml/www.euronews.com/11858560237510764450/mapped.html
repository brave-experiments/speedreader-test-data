<!DOCTYPE html><html class="no-js" lang="en" ><head><link rel="stylesheet" href="/readermode.css" type="text/css"></head><article id="article"><body class="
            t-program--web-digital-stories
        " dir="auto"><article id="article-wrapper" class="c-article__full_article"><header class="o-article__header"><div class="c-article-labels"><p class="media__body__label"><a class="media__body__cat" href="https://www.euronews.com/programs/web-digital-stories">World</a></p></div><meta content="International Women&#039;s Day: how can algorithms be sexist?"><div class="o-article__title"><h1 class="c-article-title">International Women&#039;s Day: how can algorithms be sexist?</h1></div><div class="c-article-meta"><div class="c-article-meta__info">
            
    By&nbsp;<a target="_blank" href="https://twitter.com/marta_rodguez"><b>Marta Rodriguez Martinez</b></a>
    &nbsp;&amp;&nbsp;<b>Julie Gaubert</b><time class="c-article-date" data-timestamp="1583659848" datetime="2020-03-08"><span class="c-article-date__bullet">&nbsp;&nbsp;&bullet;&nbsp;&nbsp;</span><span class="c-article-date__last-updated-label">last updated:</span>&nbsp;08/03/2020
            -&nbsp;13:44
    </time></div></div><div class="jsTopArticle c-article-media js-article-video" data-sponsored="Advertisement Feature"><div class="jsTopArticleMain"><img class="c-article-media__img"
  src="https://static.euronews.com/articles/stories/04/55/08/92/602x338_cmsv2_93a93463-86d4-5e34-8338-e7d854b04c90-4550892.jpg"
  srcset="https://static.euronews.com/articles/stories/04/55/08/92/320x180_cmsv2_93a93463-86d4-5e34-8338-e7d854b04c90-4550892.jpg 320w, https://static.euronews.com/articles/stories/04/55/08/92/602x338_cmsv2_93a93463-86d4-5e34-8338-e7d854b04c90-4550892.jpg 602w, https://static.euronews.com/articles/stories/04/55/08/92/648x364_cmsv2_93a93463-86d4-5e34-8338-e7d854b04c90-4550892.jpg 648w, https://static.euronews.com/articles/stories/04/55/08/92/773x435_cmsv2_93a93463-86d4-5e34-8338-e7d854b04c90-4550892.jpg 773w"
  sizes="(max-width: 768px) 100vw, (max-width: 1024px) 100vw, (max-width: 1280px) 100vw, 100vw"
 title=" A peep behind the scenes at the “Electronic Brain’s” multiplying unit at Welwyn Garden City, England on Nov. 6, 1946." alt=" A peep behind the scenes at the “Electronic Brain’s” multiplying unit at Welwyn Garden City, England on Nov. 6, 1946."></div></div><div class="c-article-media-copyright"><span>
     A peep behind the scenes at the “Electronic Brain’s” multiplying unit at Welwyn Garden City, England on Nov. 6, 1946.
</span><span class="c-article__copyright-separator">&nbsp;&nbsp;-&nbsp;&nbsp;</span><div class="c-article__copyright"><div class="c-article__copyright__label"> Copyright </div><span class="c-article__copyright__link"> Eddie Worth/1946 AP  </span></div></div></header><section class="row collapse jsBottomArticle u-overflow-visible" data-nid="1040352" data-cid="4552094" data-related="2"><div class="column small-12 medium-10 xlarge-11 js-responsive-iframes-container"><div class="c-article-content  js-article-content article__content"><div class="c-article-content__first-element"></div><p>Even though the first person to write an algorithm was a woman in the 19th century, artificial intelligence may now be discriminating against women.</p><div class="show-for-small-only "><div class="enw-MPU "><div id="adzone-mpu_1" class="enw-MPU__img advertising js-adzone"></div></div></div><p>Two centuries on from the first example, algorithms "have the ability to push us back decades" in gender parity, explains Susan Levy, a researcher at University College Dublin who is part of a project to prevent Artificial Intelligence algorithms from learning gender bias.</p><div id="adzone-outstream" class="advertising advertising--no-label js-adzone"></div><p>"They can exacerbate toxic masculinity and the attitudes we have been fighting for decades in society," she adds.</p><h2>The burden of history</h2><p>Artificial Intelligence (AI) learns from data that is made available, and most of it is biased, says Levy.</p><p>The problem is that machines learn from data from the last 10 to 20 years which can unwittingly  reproduce prejudices from the past. Moreover, without incorporating more recent social advances regarding gender and attitudes, the language and phraseology used in the data can perpetuate out-of-date stereotypes.  </p><p>For example, most AI hasn't heard about the global feminist movement #MeToo or the Chilean anthem "a rapist in your path."</p><div class="advertising advertising--no-label advertising--sharethrough js-adzone" id="adzone-sharethrough" data-ad-key="strnativekey" data-ad-key-value="1G96EdRgcHV6pyWWMGCR7SB7"></div><p>"We continue repeating the mistakes of the past," says the researcher.</p><p>And this bias in programming has an impact on the daily life of all women: from job searches to security checkpoints at airports.</p><div data-stories-id="4350568,3758542" data-event="widget_related" class="widget widget--type-related widget--size-fullwidth widget--align-center"><div class="widget__wrapper"><div class="widget__ratio widget__ratio--auto"><div class="widget__contents"><ul class="widget__related_list"><li class="widget__related_listItem"><a href="https://www.euronews.com/2019/03/28/how-to-fight-sexism-in-the-metoo-era-council-of-europe-starts-with-defining-it">How to fight sexism in the  #MeToo era? Council of Europe starts with defining it</a></li><li class="widget__related_listItem"><a href="https://www.euronews.com/2019/12/07/a-rapist-in-your-path-this-chilean-feminist-anthem-is-spreading-around-the-world">&lsquo;A rapist in your path&rsquo;: This Chilean feminist anthem is spreading around the world</a></li></ul></div></div></div></div><h2>Pioneers in the world of programming</h2><p>Ada Lovelace (1815-1852) became the first programmer in history, a century before computers were invented.</p><p>In the mid-nineteenth century, the British mathematician wrote what is considered to be the first algorithm for a computer machine devised by her husband, scientist Charles Babbage.</p><p>In fact, many of the pioneers in the world of programming were women. They were considered better at minute tasks - such as the ENIAC programming (an acronym for Electronic Numeric Integrator and Computer).</p><p>As part of a secret US Army project in World War II, six women programmed the first electronic computer. However, their names were omitted when it was presented to the public in 1946.</p><div class="widget widget--type-image widget--size-fullwidth widget--animation-fade-in widget--align-center"><div class="widget__wrapper"><div class="widget__ratio widget__ratio--auto"><div class="widget__contents"><figure class="widget__figure"><img class="widgetImage__image" loading="lazy" data-src="https://static.euronews.com/articles/stories/04/55/08/92/808x622_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg" src="https://static.euronews.com/articles/stories/04/55/08/92/808x622_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg" srcset="https://static.euronews.com/articles/stories/04/55/08/92/202x155_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 202w, https://static.euronews.com/articles/stories/04/55/08/92/266x205_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 266w, https://static.euronews.com/articles/stories/04/55/08/92/404x311_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 404w, https://static.euronews.com/articles/stories/04/55/08/92/534x411_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 534w, https://static.euronews.com/articles/stories/04/55/08/92/606x466_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 606w, https://static.euronews.com/articles/stories/04/55/08/92/808x622_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 808w" data-srcset="https://static.euronews.com/articles/stories/04/55/08/92/202x155_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 202w, https://static.euronews.com/articles/stories/04/55/08/92/266x205_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 266w, https://static.euronews.com/articles/stories/04/55/08/92/404x311_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 404w, https://static.euronews.com/articles/stories/04/55/08/92/534x411_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 534w, https://static.euronews.com/articles/stories/04/55/08/92/606x466_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 606w, https://static.euronews.com/articles/stories/04/55/08/92/808x622_cmsv2_6f0fb96c-125e-5a17-8af4-9d64e2837832-4550892.jpg 808w" alt="AP"><figcaption class="widget__caption"><span class="widget__captionWrap"><span class="widget__captionText">U.S. Army Maj. Gen. Gladeon M. Barnes, Dr. John G. Brainerd, and Dr. John W. Mauchly observe the function table of the ENIAC supercomputer on Feb. 9, 1946.</span><span class="widget__captionCredit">AP</span></span></figcaption></figure></div></div></div></div><h2>Male overrepresentation in sciences and technology</h2><p>The programming sector became more male-dominated in the 1980s. Even today, 59 per cent of Europe's scientists and engineers are men, according to the <a href="https://ec.europa.eu/eurostat/web/products-eurostat-news/-/EDN-20190211-1">latest Eurostat data</a>.</p><p>This inequality was inadvertently and unconsciously integrated into the writing of the algorithms.</p><p>"There is a big problem of gender disparity, especially in the machine learning process," underlines Levy. "This means that there is a lack of critical perspective."</p><p>But is it possible for male programmers to pinpoint exactly the bias they are bringing into their work?</p><p>"I don't think most engineers want to develop algorithms that discriminate based on gender or race," Levy says.</p><p>But it is not just a matter of intentions. According to Levy, it is best to have diverse programming teams to prevent machines from absorbing prejudice in the first place. "We know that teams that are not diverse do not produce good results."</p><p>Levy also recommends that tech-companies have their products tested by female members of their teams.</p><div class="widget widget--type-image widget--size-fullwidth widget--animation-fade-in widget--align-center"><div class="widget__wrapper"><div class="widget__ratio widget__ratio--auto"><div class="widget__contents"><figure class="widget__figure"><img class="widgetImage__image" loading="lazy" data-src="https://static.euronews.com/articles/stories/04/55/08/92/808x539_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg" src="https://static.euronews.com/articles/stories/04/55/08/92/808x539_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg" srcset="https://static.euronews.com/articles/stories/04/55/08/92/202x135_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 202w, https://static.euronews.com/articles/stories/04/55/08/92/266x177_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 266w, https://static.euronews.com/articles/stories/04/55/08/92/404x269_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 404w, https://static.euronews.com/articles/stories/04/55/08/92/534x356_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 534w, https://static.euronews.com/articles/stories/04/55/08/92/606x404_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 606w, https://static.euronews.com/articles/stories/04/55/08/92/808x539_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 808w" data-srcset="https://static.euronews.com/articles/stories/04/55/08/92/202x135_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 202w, https://static.euronews.com/articles/stories/04/55/08/92/266x177_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 266w, https://static.euronews.com/articles/stories/04/55/08/92/404x269_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 404w, https://static.euronews.com/articles/stories/04/55/08/92/534x356_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 534w, https://static.euronews.com/articles/stories/04/55/08/92/606x404_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 606w, https://static.euronews.com/articles/stories/04/55/08/92/808x539_cmsv2_562d0741-1d13-542a-b1d1-e9ebbbf2e11c-4550892.jpg 808w" alt="AP/Ahmad Seir"><figcaption class="widget__caption"><span class="widget__captionWrap"><span class="widget__captionText">Afghan coders practice at the Code to Inspire computer training centre in Herat province, western Afghanistan, January, 2018.</span><span class="widget__captionCredit">AP/Ahmad Seir</span></span></figcaption></figure></div></div></div></div><h2>How do algorithms discriminate against women?</h2><p>"Al and other algorithmic technologies now shape our lives in ways both significant and mundane," explains Joy Lisi Rankin, a leading researcher on gender, race and power in artificial intelligence at the AI &#8203;&#8203;Now Institute in New York.</p><p>"We rarely understand this because the technology is invisible to us, and the way it works is not at all transparent," she continues.</p><p>These algorithm systems determine, for example, "who has access to important resources and benefits," she adds.</p><p>One of the best-known cases of discrimination based on the use of AI was Amazon's attempt at automating their recruitment system.</p><p>In 2018, it came to light that the American multinational had discarded its AI tool, with which it had been selecting candidates applying for jobs for four years, because it was sexist.</p><p>Amazon's computer models had been trained by to vet applicants by looking at patterns in CVs submitted over the ten previous years. However, considering it is a male-dominated industry, most of these resumes were men's, creating a machine-learned-bias that favoured male applicants.</p><p>"Resume selection is a very problematic area," observes Levy. "Even if you tell AI algorithms not to look at gender, they will find other ways to find out."</p><p>Amazon's algorithm penalised resumes that included words related to the female gender, even in the hobbies of the candidates, such as "captain of the women's rugby team."</p><p>It is not only a matter of gender, but this type of algorithm also punishes any kind of diversity, by privileging a series of patterns that end up favouring the most privileged and represented part of society: white men.</p><p>Facial recognition systems are another problematic algorithm, explains the researcher. "If you are a woman with dark skin, it will work worse."</p><div class="widget widget--type-image widget--size-fullwidth widget--animation-fade-in widget--align-center"><div class="widget__wrapper"><div class="widget__ratio widget__ratio--auto"><div class="widget__contents"><figure class="widget__figure"><img class="widgetImage__image" loading="lazy" data-src="https://static.euronews.com/articles/stories/04/55/08/92/808x539_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg" src="https://static.euronews.com/articles/stories/04/55/08/92/808x539_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg" srcset="https://static.euronews.com/articles/stories/04/55/08/92/202x135_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 202w, https://static.euronews.com/articles/stories/04/55/08/92/266x177_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 266w, https://static.euronews.com/articles/stories/04/55/08/92/404x269_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 404w, https://static.euronews.com/articles/stories/04/55/08/92/534x356_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 534w, https://static.euronews.com/articles/stories/04/55/08/92/606x404_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 606w, https://static.euronews.com/articles/stories/04/55/08/92/808x539_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 808w" data-srcset="https://static.euronews.com/articles/stories/04/55/08/92/202x135_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 202w, https://static.euronews.com/articles/stories/04/55/08/92/266x177_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 266w, https://static.euronews.com/articles/stories/04/55/08/92/404x269_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 404w, https://static.euronews.com/articles/stories/04/55/08/92/534x356_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 534w, https://static.euronews.com/articles/stories/04/55/08/92/606x404_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 606w, https://static.euronews.com/articles/stories/04/55/08/92/808x539_cmsv2_7870dce6-67d7-5e66-89b3-1ccc1b522cac-4550892.jpg 808w" alt="AP/Darko Vojinovic"><figcaption class="widget__caption"><span class="widget__captionWrap"><span class="widget__captionText">Members of a women rights group paint a face with makeup to confuse the Huawei surveillance video cameras with face-recognition software in Belgrade, Serbia, Sept. 2019</span><span class="widget__captionCredit">AP/Darko Vojinovic</span><span class="widget__sourceLink">Darko Vojinovic</span></span></figcaption></figure></div></div></div></div><p>The consequence may be slight, for example, if your phone costs more to unlock with your face, but it can also mean having difficulties when passing a security check.</p><p>"If you are a white man who goes to an airport, you get a quick pass, if you are a woman with dark skin you have a much better chance of waiting in a longer line."</p><p>Another area in which sexist algorithms are having a crucial impact on women's lives is search engines and social networks.</p><p>"They categorise and treat users differently taking into account a series of stereotypes," says Levy. The "most dangerous" is when they use these ratings to send personalised advertising, particularly to young people who are more easily influenced, notes Levy.</p><p>Rankin gives us an example: "Facebook posted ads for better-paid jobs to white men, while women and people of colour were shown ads for less well-paid jobs. Google searches for 'black girls' or 'Latinas' produced sexist and pornographic results."</p><h2>What if they help us fight sexism instead of perpetuating it?</h2><p>The new European Union regulations for the development of AI indicate "growing awareness," says Levy.</p><p>It is an issue that the European Commission under Ursula von der Leyen is promoting and prioritisin. "It may take us 10 years," she adds.  </p><p>For this to happen, she says that multidisciplinary teams must be involved, with the perspectives of both men and women represented on the teams.</p><p>The algorithms could also help us fight discrimination, including sexism in recruitment processes. A machine could be more impartial than a human when selecting candidates - if it has learned to be inclusive.</p><p>"I have to see more evidence of this to believe it," Levy notes, adding that she does believe algorithms have the potential to be unbiased.</p></div></div></section></article></body></article></html>
